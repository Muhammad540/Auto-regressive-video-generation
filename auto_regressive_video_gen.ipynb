{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vowXRstocd8F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/training_video.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "video_data = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    video_data.append(gray_frame)\n",
        "\n",
        "cap.release()\n",
        "video_data = np.array(video_data)\n",
        "print(\"Video data shape:\", video_data.shape)"
      ],
      "metadata": {
        "id": "K4dXxwsOEDD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patches(frame, patch_size=16):\n",
        "    \"\"\"\n",
        "    frame: 2D NumPy array of shape (H, W), e.g. (128, 128).\n",
        "    patch_size: size of each square patch.\n",
        "\n",
        "    Returns:\n",
        "      A NumPy array of shape (num_patches, patch_dim),\n",
        "      where num_patches = (H/patch_size)*(W/patch_size),\n",
        "      and patch_dim = patch_size*patch_size.\n",
        "    \"\"\"\n",
        "    H, W = frame.shape\n",
        "    patches = []\n",
        "    for i in range(0, H, patch_size):\n",
        "        for j in range(0, W, patch_size):\n",
        "            patch = frame[i:i+patch_size, j:j+patch_size]\n",
        "            patches.append(patch.reshape(-1))  # flatten 16×16 -> 256\n",
        "    # shape: (64, 256) if H=128, W=128, patch_size=16\n",
        "    return np.stack(patches, axis=0)\n",
        "\n",
        "\n",
        "def build_dataset_patches(video_data, context_length=3, patch_size=16):\n",
        "    \"\"\"\n",
        "    video_data: NumPy array of shape (T, H, W), e.g. (11072, 128, 128).\n",
        "    context_length: number of previous frames' patches to include as input.\n",
        "    patch_size: dimension of each square patch.\n",
        "\n",
        "    Returns:\n",
        "        X: shape (T, context_length*64, 256),\n",
        "           i.e. for each time t, we have N frames × 64 patches/frame = N*64 patches,\n",
        "           each patch is 256-d if patch_size=16.\n",
        "        Y: shape (T, 64, 256),\n",
        "           i.e. the patches for the \"current\" frame t.\n",
        "    \"\"\"\n",
        "    T, H, W = video_data.shape\n",
        "\n",
        "    # For 128x128 with 16x16 patches, you get 64 patches per frame.\n",
        "    # Each patch is 16x16=256 floats when flattened.\n",
        "    num_patches = (H // patch_size) * (W // patch_size)  # e.g. 64\n",
        "    patch_dim = patch_size * patch_size                  # e.g. 256\n",
        "\n",
        "    # We'll store inputs and targets for each frame\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    # Initialize context with zero patches for the first 'context_length' frames\n",
        "    zero_patches = np.zeros((num_patches, patch_dim), dtype=np.float32)\n",
        "    context = [zero_patches] * context_length\n",
        "\n",
        "    for t in range(T):\n",
        "        # Extract patches for the current frame\n",
        "        curr_patches = extract_patches(video_data[t], patch_size=patch_size).astype(np.float32)\n",
        "\n",
        "        # Concatenate the patches from the previous N frames along the patch dimension\n",
        "        # shape: (N*64, 256)\n",
        "        x_t = np.concatenate(context, axis=0)  # context_length*64 rows, each 256-d\n",
        "        y_t = curr_patches  # shape (64, 256) if predicting the \"current\" frame\n",
        "\n",
        "        inputs.append(x_t)\n",
        "        targets.append(y_t)\n",
        "\n",
        "        # Shift context: drop the oldest frame's patches, add the current frame's patches\n",
        "        context = context[1:] + [curr_patches]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X = torch.tensor(np.stack(inputs, axis=0))   # shape (T, N*64, 256)\n",
        "    Y = torch.tensor(np.stack(targets, axis=0))  # shape (T, 64, 256)\n",
        "\n",
        "    print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "X_all, Y_all = build_dataset_patches(video_data, context_length=3, patch_size=16)\n",
        "# e.g. X_all shape: (10, 3*64, 256) = (10, 192, 256)\n",
        "#      Y_all shape: (10, 64, 256)\n",
        "\n",
        "# Then split into train/dev/test\n",
        "n1 = int(0.8 * len(X_all))\n",
        "n2 = int(0.9 * len(X_all))\n",
        "Xtr, Ytr = X_all[:n1], Y_all[:n1]\n",
        "Xdev, Ydev = X_all[n1:n2], Y_all[n1:n2]\n",
        "Xte, Yte = X_all[n2:], Y_all[n2:]\n",
        "\n",
        "print(\"Train shapes:\", Xtr.shape, Ytr.shape)\n",
        "print(\"Dev shapes:\", Xdev.shape, Ydev.shape)\n",
        "print(\"Test shapes:\", Xte.shape, Yte.shape)"
      ],
      "metadata": {
        "id": "MB5XgJxdczFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# Hyperparameters\n",
        "# ---------------------\n",
        "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
        "\n",
        "context_length = 3           # number of past frames\n",
        "n_patches = 64               # e.g., 128x128 frames split into 16x16 patches => 64 patches\n",
        "patch_dim = 256              # each patch is 16x16 => 256 pixels\n",
        "embedding_dim = 64           # dimension for patch embedding\n",
        "hidden_dim = 400             # dimension of hidden layer\n",
        "output_dim = n_patches * patch_dim  # 64 * 256 = 16384 (predict next frame's patches)\n",
        "\n",
        "# ---------------------\n",
        "# Parameter Shapes\n",
        "# ---------------------\n",
        "# C: (patch_dim, embedding_dim) => 256 -> 64 projection for each patch\n",
        "C = torch.randn((patch_dim, embedding_dim), generator=g) * 0.01\n",
        "\n",
        "# MLP input: context_length * n_patches * embedding_dim\n",
        "# e.g. 3 frames * 64 patches * 64 embedding = 3*64*64 = 12288\n",
        "mlp_input_dim = context_length * n_patches * embedding_dim\n",
        "\n",
        "# MLP output: next frame's 64 patches * 256 pixels/patch\n",
        "# => 16384 floats\n",
        "mlp_output_dim = output_dim\n",
        "\n",
        "# W1, b1 define the hidden layer\n",
        "W1 = torch.randn((mlp_input_dim, hidden_dim), generator=g) * 0.01\n",
        "b1 = torch.zeros(hidden_dim)\n",
        "\n",
        "# W2, b2 define the output layer\n",
        "W2 = torch.randn((hidden_dim, mlp_output_dim), generator=g) * 0.01\n",
        "b2 = torch.zeros(mlp_output_dim)\n",
        "\n",
        "# Put all parameters in a list for easy zero_grad / updates\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "EAA0_oigc13X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.nelement() for p in parameters) # number of parameters in total"
      ],
      "metadata": {
        "id": "_HcVef6-c5S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "coNBea1sc6tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "X shape: torch.Size([11072, 192, 256]) Y shape: torch.Size([11072, 64, 256])\n",
        "Train shapes: torch.Size([8857, 192, 256]) torch.Size([8857, 64, 256])\n",
        "Dev shapes: torch.Size([1107, 192, 256]) torch.Size([1107, 64, 256])\n",
        "Test shapes: torch.Size([1108, 192, 256]) torch.Size([1108, 64, 256])\n",
        "C: 256x64\n",
        "W: 12288x400\n",
        "'''"
      ],
      "metadata": {
        "id": "OunnOV16St0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X_batch):\n",
        "    \"\"\"\n",
        "    X_batch: (batch_size, 192, 256)\n",
        "        where 192 = context_length * 64 patches,\n",
        "              256 = flattened patch size.\n",
        "    Returns:\n",
        "        logits: (batch_size, 16384)\n",
        "        This is the predicted next frame (64 patches of 256 floats each).\n",
        "    \"\"\"\n",
        "    # 1) Embed each patch: shape => (batch_size, 192, 64)\n",
        "    emb = X_batch @ C  # C is (256, 64)\n",
        "\n",
        "    # 2) Flatten embedded patches: shape => (batch_size, 192*64) = (batch_size, 12288)\n",
        "    emb_flat = emb.reshape(emb.shape[0], -1)\n",
        "\n",
        "    # 3) Hidden layer\n",
        "    h = torch.tanh(emb_flat @ W1 + b1)  # shape => (batch_size, 200)\n",
        "\n",
        "    # 4) Output layer -> next frame's pixels\n",
        "    logits = h @ W2 + b2  # shape => (batch_size, 16384)\n",
        "\n",
        "    return logits\n",
        "\n",
        "num_epochs = 10000\n",
        "batch_size = 32\n",
        "lr = 0.1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Sample a random minibatch\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "    X_batch = Xtr[ix]  # shape (batch_size, 192, 256)\n",
        "    Y_batch = Ytr[ix]  # shape (batch_size, 64, 256)\n",
        "\n",
        "    logits = forward(X_batch)   # shape (batch_size, 16384)\n",
        "\n",
        "    Y_batch_flat = Y_batch.reshape(Y_batch.shape[0], -1)\n",
        "\n",
        "    loss = F.mse_loss(logits, Y_batch_flat)\n",
        "\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    for p in parameters:\n",
        "        p.data -= lr * p.grad\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "jiKD-PPOdClh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loss\n",
        "logits = forward(Xtr)\n",
        "loss = F.mse_loss(logits, Ytr.reshape(Ytr.shape[0], -1))\n",
        "loss"
      ],
      "metadata": {
        "id": "ZlhYinNgdIVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "logits = forward(Xdev)\n",
        "loss = F.mse_loss(logits, Ydev.reshape(Ytr.shape[0], -1))\n",
        "loss"
      ],
      "metadata": {
        "id": "cJH3DDu4dLTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test loss\n",
        "logits = forward(Xte)\n",
        "loss = F.mse_loss(logits, Yte.reshape(Ytr.shape[0], -1))\n",
        "loss"
      ],
      "metadata": {
        "id": "DwOgCg1vdNhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpatchify(patches, patch_size=16, frame_size=128):\n",
        "    \"\"\"\n",
        "    patches: shape (64, 256) => 64 patches, each 16x16=256 pixels\n",
        "    Returns: A (128,128) 2D array\n",
        "    \"\"\"\n",
        "    # (8 x 8) grid of patches, each (16 x 16)\n",
        "    out_frame = np.zeros((frame_size, frame_size), dtype=np.float32)\n",
        "    idx = 0\n",
        "    for i in range(0, frame_size, patch_size):\n",
        "        for j in range(0, frame_size, patch_size):\n",
        "            patch = patches[idx].reshape(patch_size, patch_size)\n",
        "            out_frame[i:i+patch_size, j:j+patch_size] = patch\n",
        "            idx += 1\n",
        "    return out_frame\n",
        "\n",
        "def predict_next_frame(context_frames,\n",
        "                       C, W1, b1, W2, b2,\n",
        "                       patch_size=16, embed_dim=64, hidden_dim=200):\n",
        "    \"\"\"\n",
        "    context_frames: shape (N, 64, 256) in numpy or torch (the last N frames, each 64 patches).\n",
        "    Returns: predicted_patches, shape (64, 256), as a torch tensor\n",
        "    \"\"\"\n",
        "    # 1) Merge the N frames into one shape: (N*64, 256)\n",
        "    #    Then add batch dimension (1, N*64, 256)\n",
        "    ctx = torch.tensor(context_frames, dtype=torch.float32)\n",
        "    ctx = ctx.view(1, -1, 256)  # shape (1, N*64, 256)\n",
        "\n",
        "    # 2) Embedding: (1, N*64, 256) @ (256, embed_dim) => (1, N*64, embed_dim)\n",
        "    emb = ctx @ C\n",
        "\n",
        "    # 3) Flatten: => (1, N*64*embed_dim)\n",
        "    emb_flat = emb.reshape(1, -1)\n",
        "\n",
        "    # 4) Hidden layer\n",
        "    h = torch.tanh(emb_flat @ W1 + b1)\n",
        "\n",
        "    # 5) Output layer => (1, 64*256)\n",
        "    out = h @ W2 + b2\n",
        "\n",
        "    # 6) Reshape => (64, 256)\n",
        "    predicted_patches = out.view(64, 256)\n",
        "    return predicted_patches\n",
        "\n",
        "def sample_frames(initial_frames,\n",
        "                  C, W1, b1, W2, b2,\n",
        "                  num_generate=20,\n",
        "                  context_length=3):\n",
        "    \"\"\"\n",
        "    initial_frames: list (or np array) of at least `context_length` frames,\n",
        "                    each shape (64, 256) in patch form (like your Y or dataset).\n",
        "    Returns: list of predicted frames in patch form.\n",
        "    \"\"\"\n",
        "    generated = []\n",
        "\n",
        "    # Start with the last `context_length` frames from initial_frames as context\n",
        "    context_buffer = list(initial_frames[-context_length:])\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        # 1) Predict next frame from context_buffer\n",
        "        predicted_patches = predict_next_frame(\n",
        "            context_buffer, C, W1, b1, W2, b2\n",
        "        )\n",
        "\n",
        "        # 2) Convert to numpy (if needed) and store\n",
        "        pred_np = predicted_patches.detach().cpu().numpy()\n",
        "        generated.append(pred_np)\n",
        "\n",
        "        # 3) Update context: drop oldest, append new\n",
        "        context_buffer = context_buffer[1:] + [pred_np]\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Example usage:\n",
        "# Suppose you have some list of frames_in_patches (each (64,256)) from your dataset\n",
        "# as a \"seed\" context.\n",
        "seed_frames = [Ytr[0].numpy(), Ytr[1].numpy(), Ytr[2].numpy()]\n",
        "\n",
        "# Generate frames\n",
        "generated_frames = sample_frames(\n",
        "    seed_frames, C, W1, b1, W2, b2,\n",
        "    num_generate=200, context_length=3\n",
        ")"
      ],
      "metadata": {
        "id": "TQUMmgRrdRIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}